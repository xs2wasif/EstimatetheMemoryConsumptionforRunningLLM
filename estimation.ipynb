{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f05d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Hugging Face caching paths\n",
    "os.environ[\"HF_HOME\"] = \"/data/models\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/data/models\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/data/models/datasets\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"false\"\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"True\"\n",
    "\n",
    "# Auto-create directories if they don't exist\n",
    "os.makedirs(\"/data/models\", exist_ok=True)\n",
    "os.makedirs(\"/data/models/datasets\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965333f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen3-VL-235B-A22B-Thinking\" # @param {type:\"string\"}\n",
    "model_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# hidden_layers = model_config.num_hidden_layers\n",
    "# hidden_size =  model_config.hidden_size\n",
    "# attention_heads = model_config.num_attention_heads\n",
    "# kv_heads = 0\n",
    "# if hasattr(model_config, \"num_key_value_heads\"):\n",
    "#   kv_heads = model_config.num_key_value_heads\n",
    "\n",
    "# print(\"Model: \"+str(model_name))\n",
    "# print(\"Hidden layers (L): \"+str(hidden_layers))\n",
    "# print(\"Hidden size (h): \"+str(hidden_size))\n",
    "# print(\"Attention heads (a): \"+str(attention_heads))\n",
    "# if kv_heads > 0:\n",
    "#   print(\"Key-value heads (g): \"+str(kv_heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60f2be",
   "metadata": {},
   "source": [
    "MoE MODEL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b60b709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-VL-235B-A22B-Thinking\n",
      "Hidden layers (L): 94\n",
      "Hidden size (h): 4096\n",
      "Attention heads (a): 64\n",
      "Key-value heads (g): 4\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = model_config.text_config.num_hidden_layers\n",
    "hidden_size =  model_config.text_config.hidden_size\n",
    "attention_heads = model_config.text_config.num_attention_heads\n",
    "kv_heads = 0\n",
    "if hasattr(model_config.text_config, \"num_key_value_heads\"):\n",
    "  kv_heads = model_config.text_config.num_key_value_heads\n",
    "print(\"Model: \"+str(model_name))\n",
    "print(\"Hidden layers (L): \"+str(hidden_layers))\n",
    "print(\"Hidden size (h): \"+str(hidden_size))\n",
    "print(\"Attention heads (a): \"+str(attention_heads))\n",
    "if kv_heads > 0:\n",
    "  print(\"Key-value heads (g): \"+str(kv_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b136fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model (n): 235B\n",
      "Bitwidth of the model's parameters (p): 16-bit\n",
      "Sequence length (s): 256000\n",
      "Batch size (b): 16\n",
      "Use FlashAttention: Yes\n",
      "Use a KV cache: Yes\n"
     ]
    }
   ],
   "source": [
    "#Number of parameters in the model (in billions)\n",
    "nb_billion_parameters = 235 # @param {type:\"number\"}\n",
    "print(\"Number of parameters in the model (n): \"+str(nb_billion_parameters)+\"B\")\n",
    "\n",
    "#Precision of the parameters in the model\n",
    "bitwidth_model = 16 # @param {type:\"integer\"}\n",
    "print(\"Bitwidth of the model's parameters (p): \"+str(bitwidth_model)+\"-bit\")\n",
    "\n",
    "#The maximum number of tokens in a sequence\n",
    "seqlen = 256000 # @param {type:\"integer\"}\n",
    "print(\"Sequence length (s): \"+str(seqlen))\n",
    "\n",
    "#The batch size\n",
    "batch_size = 16 # @param {type:\"integer\"}\n",
    "print(\"Batch size (b): \"+str(batch_size))\n",
    "\n",
    "\n",
    "\n",
    "#Use FlashAttention\n",
    "Flash_Attention = True # @param {type:\"boolean\"}\n",
    "tile_size = 128\n",
    "if Flash_Attention:\n",
    "  print(\"Use FlashAttention: Yes\")\n",
    "else:\n",
    "  print(\"Use FlashAttention: No\")\n",
    "\n",
    "#Use a KV cache (if yes, should be equal to the seqlen)\n",
    "Use_KV_Cache = True # @param {type:\"boolean\"}\n",
    "kv_cache = 0\n",
    "if Use_KV_Cache:\n",
    "  print(\"Use a KV cache: Yes\")\n",
    "  kv_cache = seqlen\n",
    "else:\n",
    "  print(\"Use a KV cache: No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a12d445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory consumption of the model: 470.0 GB\n",
      "\n",
      "Memory consumption of vanilla inference: 537944.65 GB \n",
      "\n",
      "Memory consumption of inference with GQA: 34498.15 GB \n",
      "\n",
      "Memory consumption of inference with FlashAttention: 1077.94 GB \n",
      "\n",
      "Memory consumption of the KV cache (with GQA): 1577.06 GB \n",
      "\n",
      "Total Memory consumption (given the selected configuration): 3125.0 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def estimate_consumption_inference():\n",
    "  return round((32*seqlen*batch_size*hidden_size + 4*attention_heads*seqlen*seqlen*batch_size)*2/(1000**3),2)\n",
    "def estimate_consumption_inference_gqa():\n",
    "  return round((28*seqlen*batch_size*hidden_size + ((2*kv_heads)/attention_heads)*seqlen*batch_size*hidden_size + 4*kv_heads*seqlen*seqlen*batch_size)*2/(1000**3),2)\n",
    "\n",
    "def estimate_consumption_inference_FA(): #Ignoring GQA for simplicity; will be slightly lower with GQA\n",
    "  return round((32*seqlen*batch_size*hidden_size + 4*tile_size*seqlen*batch_size)*2/(1000**3),2)\n",
    "\n",
    "\n",
    "def kv_cache():\n",
    "  return round(2*hidden_layers*seqlen*batch_size*hidden_size*2/(1000**3),2)\n",
    "\n",
    "def kv_cache_gqa():\n",
    "  return round(2*hidden_layers*seqlen*batch_size*(hidden_size/kv_heads)*2/(1000**3),2)\n",
    "\n",
    "def estimate_model_size():\n",
    "  return round(nb_billion_parameters*bitwidth_model/8*(1000**3)/(1000**3),2)\n",
    "\n",
    "\n",
    "activation_consumption_inference = estimate_consumption_inference()\n",
    "activation_consumption_inference_gqa = estimate_consumption_inference_gqa()\n",
    "activation_consumption_inference_FA = estimate_consumption_inference_FA()\n",
    "model_consumption = estimate_model_size()\n",
    "\n",
    "print(\"Memory consumption of the model: \"+str(model_consumption)+\" GB\\n\")\n",
    "\n",
    "print(\"Memory consumption of vanilla inference: \"+str(activation_consumption_inference)+\" GB \\n\")\n",
    "if kv_heads > 0:\n",
    "  print(\"Memory consumption of inference with GQA: \"+str(activation_consumption_inference_gqa)+\" GB \\n\")\n",
    "\n",
    "print(\"Memory consumption of inference with FlashAttention: \"+str(activation_consumption_inference_FA)+\" GB \\n\")\n",
    "\n",
    "if Use_KV_Cache:\n",
    "  if kv_heads > 0:\n",
    "    kv_cache_cost = kv_cache_gqa()\n",
    "    print(\"Memory consumption of the KV cache (with GQA): \"+str(kv_cache_cost)+\" GB \\n\")\n",
    "  else:\n",
    "    kv_cache_cost = kv_cache()\n",
    "    print(\"Memory consumption of the KV cache: \"+str(kv_cache_cost)+\" GB \\n\")\n",
    "else:\n",
    "  kv_cache_cost = 0\n",
    "\n",
    "if Flash_Attention:\n",
    "  print(\"Total Memory consumption (given the selected configuration): \"+str(round(model_consumption+kv_cache_cost+activation_consumption_inference_FA,2))+\" GB\\n\")\n",
    "elif kv_heads > 0:\n",
    "  print(\"Total Memory consumption (given the selected configuration): \"+str(round(model_consumption+kv_cache_cost+activation_consumption_inference_gqa,2))+\" GB\\n\")\n",
    "else:\n",
    "  print(\"Total Memory consumption (given the selected configuration): \"+str(round(model_consumption+kv_cache_cost+activation_consumption_inference,2))+\" GB\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
